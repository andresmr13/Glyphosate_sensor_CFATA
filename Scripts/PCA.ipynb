{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "83f6a52d",
   "metadata": {},
   "source": [
    "# Apply PCA to input data (50 emission and 50 dispersion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cba1dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "#import pandas as pd\n",
    "import csv\n",
    "\n",
    "# File path with samples\n",
    "file_path = 'sep17v2.csv'\n",
    "\n",
    "# Open file with samples\n",
    "with open(file_path, 'r') as file:\n",
    "    reader = csv.reader(file)\n",
    "    n = 0\n",
    "    data = []\n",
    "    for i,row in enumerate(reader):\n",
    "        data.append(row)\n",
    "        # if n<28:\n",
    "        #     data.append(row)\n",
    "        # n+=1\n",
    "len(data)\n",
    "\n",
    "# X and y data lists\n",
    "_inputs = []\n",
    "_targets = []\n",
    "\n",
    "# Add features to training variable\n",
    "for i in range(len(data)):\n",
    "        row_data = []\n",
    "        for j in range(3,104):\n",
    "                if j!= 53:\n",
    "                        row_data.append(float(data[i][j]))\n",
    "        _inputs.append(row_data)\n",
    "        _targets.append(float(data[i][1]))\n",
    "\n",
    "# Convert list to arrays\n",
    "X_data = np.array(_inputs)\n",
    "y_data= np.array(_targets)\n",
    "\n",
    "print(f\"Original shape of training data: {X_data.shape}\")\n",
    "\n",
    "# --- Step 1: Scale the Data ---\n",
    "# PCA is sensitive to feature scales, so we standardize the data first.\n",
    "# IMPORTANT: You fit the scaler ONLY on the training data.\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_data)\n",
    "\n",
    "# --- Step 2: Apply PCA ---\n",
    "# We'll tell PCA to automatically select enough components to explain 95% of the variance.\n",
    "pca = PCA(n_components=0.95)\n",
    "\n",
    "# Fit PCA on the scaled training data and then transform it.\n",
    "X_train_pca = pca.fit_transform(X_train_scaled)\n",
    "\n",
    "print(f\"\\nNumber of features after PCA: {pca.n_components_}\")\n",
    "print(f\"Shape of data after PCA: {X_train_pca.shape}\")\n",
    "\n",
    "# X_train_pca is your new, lower-dimensional training dataset.\n",
    "# You can now use this to train your regression or MLP model.\n",
    "\n",
    "# --- Step 3: Handling New/Test Data ---\n",
    "# When you need to make predictions on a test set (X_test), you must use\n",
    "# the SAME scaler and pca objects you already fitted.\n",
    "\n",
    "# Example:\n",
    "# X_test = ... your test data ...\n",
    "# X_test_scaled = scaler.transform(X_test)  # Note: just .transform(), not .fit_transform()\n",
    "# X_test_pca = pca.transform(X_test_scaled)    # Note: just .transform(), not .fit_transform()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gly_models",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
